---
title: "Account management team effectiveness and tooling"
date: 2026-02-11
status: planned
outcome: "Inform the design of consolidated account health tooling and streamlined SAM workflows that reduce manual reporting, surface churn risks automatically, and give leadership visibility without burdening individual contributors"
hypotheses:
  - "SAMs spend a disproportionate amount of time on manual reporting and status updates that leadership could get from existing systems"
  - "Account health is primarily assessed through relationship signals (meeting tone, responsiveness) rather than quantitative data because quantitative data is fragmented across too many tools"
  - "Higher product usage (monthly active users) correlates with lower churn risk, and SAMs intuitively know this but lack a systematic way to monitor it"
  - "Support ticket triage and resolution gaps are invisible to SAMs and product teams until the customer escalates"
participant_criteria: "Strategic Account Managers (SAMs), Account Managers, and CS leadership managing healthcare/enterprise analytics accounts"
sample_size: 6
interviews_completed: 0
---

# Account Management Team Effectiveness and Tooling

## Objective

Understand how account management teams currently assess account health, manage their workflows across fragmented tools, and communicate status to leadership -- to inform building consolidated tooling that automates churn risk detection, surfaces upsell opportunities, and reduces the manual reporting burden on SAMs.

This research will inform both product decisions (what to build in an account health system) and process decisions (how to restructure workflows so SAMs spend more time with customers and less time on internal reporting).

## Three Most Important Things to Learn

1. **Current behavior:** How do SAMs actually assess whether an account is healthy or at risk today? What signals do they use, and where do those signals live across their tool stack?
2. **Pain points:** Where do SAMs lose the most time in their weekly workflow, and what are the biggest gaps between what leadership needs to know and what's easily accessible?
3. **Desired outcomes:** What would "good" look like for SAMs and their leaders? What would change about their day-to-day if account health were automatically monitored?

## Hypotheses

| # | Hypothesis | Status |
|---|-----------|--------|
| 1 | SAMs spend a disproportionate amount of time on manual reporting and status updates that leadership could get from existing systems | UNTESTED |
| 2 | Account health is primarily assessed through relationship signals (meeting tone, responsiveness) rather than quantitative data because quantitative data is fragmented across too many tools | UNTESTED |
| 3 | Higher product usage (monthly active users) correlates with lower churn risk, and SAMs intuitively know this but lack a systematic way to monitor it | UNTESTED |
| 4 | Support ticket triage and resolution gaps are invisible to SAMs and product teams until the customer escalates | UNTESTED |

## Participant Criteria

**Include:**
- Strategic Account Managers or Account Managers who manage 5+ accounts
- CS leaders who manage teams of SAMs and need cross-account visibility
- Delivery or product managers who interact with account health data
- People who have experienced at least one churn or churn risk in the past 6 months

**Exclude:**
- Sales-only roles who don't manage ongoing accounts
- SAMs with fewer than 6 months tenure (not enough workflow patterns established)
- Offshore support staff (different tool access and workflows)

### Screener Questions

1. "How many accounts do you currently manage or oversee?"
2. "How many different tools do you log into on a typical Monday to stay on top of your accounts?"
3. "In the last 3 months, how many times has leadership asked you for an account status update that you'd already documented somewhere?"
4. "Describe the last account you considered at risk for churn -- how did you first know something was wrong?"

## Discussion Guide

### Opening (2-3 min)

- Introduce yourself and the purpose: learning about how account management works day-to-day, not evaluating anyone's performance
- "I'd love to hear about your actual experience managing accounts. There are no wrong answers -- I'm trying to understand the real workflow, not the ideal one."

### Story Elicitation (15-20 min)

**Primary story prompt:**
> "Walk me through your last Monday morning. From when you sat down at your desk, what did you do first to get up to speed on your accounts?"

**Follow-up probes:**
- "What happened next?"
- "Which tool did you open first? Why that one?"
- "How did you know where things stood with [specific account]?"
- "What were you trying to figure out?"
- "Was there anything you couldn't find or had to piece together from multiple places?"

**Second story prompt (churn/risk specific):**
> "Tell me about the last time you realized an account was in trouble. Walk me through how you first noticed."

**Follow-up probes:**
- "What was the first signal that something was off?"
- "How long had it been going on before you noticed?"
- "What did you do about it?"
- "Who else needed to know? How did you communicate it?"
- "Was there anything that could have alerted you earlier?"

### Depth Probes (5-10 min)

- "You mentioned updating [Salesforce/Jira/etc.] -- how much time do you spend on that per week?"
- "When leadership asks for a status update, what do they actually need that they can't get themselves?"
- "Has there been a time when a support ticket or product issue affected an account and you didn't know about it until the customer told you?"
- "If something could automatically flag at-risk accounts for you, what signals would you trust it to look at?"
- "Why was [that specific workflow/workaround] important to you?"

### Closing (2-3 min)

- "Is there anything about how you manage accounts that I should have asked about?"
- "If you could change one thing about your tools or process tomorrow, what would it be?"
- "Who else on the team should I talk to about this? Anyone who does things very differently from you?"

## Post-Interview Checklist

- [ ] Write interview snapshot within 24 hours (run `/workflows:research process`)
- [ ] Note top 3 surprises from this interview
- [ ] Update hypothesis status in this plan
- [ ] Identify follow-up questions for next interview
- [ ] Add new screener criteria if participant fit was imperfect

## Schedule

| # | Participant | Date | Status |
|---|-----------|------|--------|
| 1 | Krista (SAM - WellCare/Centene/SelectHealth) | 2026-02-09 | Completed (transcript available) |
| 2 | Ashley (SAM - WellCare/Centene/SelectHealth) | 2026-01-29 | Completed (transcript available) |
| 3 | TBD | TBD | Not scheduled |
| 4 | TBD | TBD | Not scheduled |
| 5 | TBD | TBD | Not scheduled |
| 6 | TBD | TBD | Not scheduled |

## Human Review Checklist

- [ ] Objective is outcome-focused (not feature-focused)
- [ ] Hypotheses are falsifiable statements about behavior
- [ ] Screener questions ask about past behavior, not opinions
- [ ] Discussion guide follows story-based structure
- [ ] No leading questions or solution pitching in guide
- [ ] Sample size appropriate for research type
